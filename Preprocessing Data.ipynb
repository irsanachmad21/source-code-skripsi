{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VQtKcVyzy8gh40xIcMrrdMzvGMvxroSS","timestamp":1745814512540}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M1ov-9tLGfDf"},"source":["# ***PREPROCESSING DATA***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bulpCiaGfDg"},"outputs":[],"source":["import pandas as pd\n","\n","data = pd.read_csv(\"dataset.csv\", on_bad_lines='warn')\n","\n","data.info()"]},{"cell_type":"code","source":["df = pd.DataFrame(data[['teks']])\n","df.head(5)"],"metadata":{"id":"OcxiNgsFXaMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ksD7KU5GfDh"},"source":["**PROSES HAPUS DATA DUPLIKAT**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPwUVT1gGfDh"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3pc5PRG97mo"},"outputs":[],"source":["df.drop_duplicates(subset =\"teks\", keep = 'first', inplace = True)\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"SeySRi9mGfDi"},"source":["**WORDCLOUD SEBELUM PREPROCESSING**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmkjpfDKGfDi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import matplotlib.pyplot as plt\n","\n","df['teks'] = df['teks'].fillna('')\n","\n","text = ' '.join(df['teks'].astype(str).tolist())\n","\n","stopwords = set(STOPWORDS)\n","\n","wc = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=500, width=800, height=400)\n","\n","wc.generate(text)\n","\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wc, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss-9Mi8WGfDi"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","text = \" \".join(df[\"teks\"])\n","\n","tokens = text.split()\n","word_counts = Counter(tokens)\n","\n","top_words = word_counts.most_common(10)\n","word, count = zip(*top_words)\n","# Menggunakan palet warna lebih soft\n","colors = plt.cm.Pastel1(range(len(word)))\n","\n","plt.figure(figsize=(15, 4))\n","bars = plt.bar(word, count, color=colors)\n","plt.xlabel(\"Kata-Kata Sering Muncul\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Jumlah Kata\", fontsize=12, fontweight='bold')\n","plt.title(\"Frekuensi Kata\", fontsize=18, fontweight='bold')\n","plt.xticks(rotation=45)\n","\n","# Menambahkan angka rata tengah di atas setiap bar\n","for bar, num in zip(bars, count):\n","    plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, num + 1, str(num), fontsize=12, color='black', ha='center')\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Jo5wRk02GfDj"},"source":["**PROSES CLEANING**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFb6swWEGfDj"},"outputs":[],"source":["import re\n","import string\n","import nltk\n","\n","# Fungsi untuk menghapus emoji\n","def remove_emoji(tweet):\n","    if tweet is not None and isinstance(tweet, str):\n","        emoji_pattern = re.compile(\"[\"\n","            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","            u\"\\U0001F004-\\U0001F0CF\"  # Additional emoticons\n","            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n","                               \"]+\", flags=re.UNICODE)\n","        return emoji_pattern.sub(r'', tweet)\n","    else:\n","        return tweet\n","\n","# Fungsi untuk menghapus simbol\n","def remove_symbols(tweet):\n","    if tweet is not None and isinstance(tweet, str):\n","        tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)  # Menghapus semua simbol\n","    return tweet\n","\n","# Fungsi untuk menghapus angka\n","def remove_numbers(tweet):\n","    if tweet is not None and isinstance(tweet, str):\n","        tweet = re.sub(r'\\d', '', tweet)  # Menghapus semua angka\n","    return tweet\n","\n","def remove_username(text):\n","    import re\n","    return re.sub(r'@[^\\s]+', '', text)\n","\n","\n","df['cleaning'] = df['teks'].apply(lambda x: remove_username(x))\n","df['cleaning'] = df['cleaning'].apply(lambda x: remove_emoji(x))\n","df['cleaning'] = df['cleaning'].apply(lambda x: remove_symbols(x))\n","df['cleaning'] = df['cleaning'].apply(lambda x: remove_numbers(x))\n","\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"WN6uug7RGfDj"},"source":["**PROSES CASE FOLDING**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIUOpaiiGfDj"},"outputs":[],"source":["def case_folding(text):\n","    if isinstance(text, str):\n","        lowercase_text = text.lower()\n","        return lowercase_text\n","    else:\n","        return text\n","\n","df['case_folding'] = df['cleaning'].apply(case_folding)\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"sBP4aBL9GfDk"},"source":["**Normalisasi Kata**\n","\n","---\n","\n"]},{"cell_type":"code","source":["# Upload kaggle.json\n","from google.colab import files\n","files.upload() # Pilih kaggle.json\n","\n","# Setup Kaggle API credentials\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Download dataset dari Kaggle\n","!kaggle datasets download -d fornigulo/kamus-slag\n","\n","# Unzip dataset\n","!unzip kamus-slag.zip\n"],"metadata":{"id":"taf4PP7QYbvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWXiovq5GfDk"},"outputs":[],"source":["import pandas as pd\n","\n","# Fungsi penggantian kata tidak baku\n","def replace_taboo_words(text, kamus_tidak_baku):\n","    if isinstance(text, str):\n","        words = text.split()\n","        replaced_words = []\n","        kalimat_baku = []\n","        kata_diganti = []\n","        kata_tidak_baku_hash = []\n","\n","        for word in words:\n","            if word in kamus_tidak_baku:\n","                baku_word = kamus_tidak_baku[word]\n","                if isinstance(baku_word, str) and all(char.isalpha() or char.isspace() for char in baku_word):\n","                    replaced_words.append(baku_word)\n","                    kalimat_baku.append(baku_word)\n","                    kata_diganti.append(word)\n","                    kata_tidak_baku_hash.append(hash(word))\n","            else:\n","                replaced_words.append(word)\n","        replaced_text = ' '.join(replaced_words)\n","    else:\n","        replaced_text = ''\n","        kalimat_baku = []\n","        kata_diganti = []\n","        kata_tidak_baku_hash = []\n","\n","    return replaced_text, kalimat_baku, kata_diganti, kata_tidak_baku_hash"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQI1AMK7GfDk"},"outputs":[],"source":["# Baca dataset\n","data =  pd.DataFrame(df[['teks','cleaning','case_folding']])\n","data.head(5)"]},{"cell_type":"code","source":["kamus_data = pd.read_excel('kamuskatabaku.xlsx')\n","kamus_tidak_baku = dict(zip(kamus_data['tidak_baku'], kamus_data['kata_baku']))\n","kamus_data.head()"],"metadata":{"id":"aXrMdrxqY_N9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qunjomAlGfDl"},"outputs":[],"source":["# Terapkan fungsi penggantian kata tidak baku\n","data['normalization'], data['Kata_Baku'], data['Kata_Tidak_Baku'],data['Kata_Tidak_Baku_Hash'] = zip(*data['case_folding'].apply(lambda x: replace_taboo_words(x, kamus_tidak_baku)))\n","\n","df =  pd.DataFrame(data[['teks','cleaning','case_folding','normalization']])\n","\n","df.head(100)"]},{"cell_type":"markdown","metadata":{"id":"I4PLWSmRGfDl"},"source":["**TOKENIZATION**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wepcILNGfDl"},"outputs":[],"source":["def tokenize(text):\n","    tokens = text.split()\n","    return tokens\n","\n","df['tokenize'] = df['normalization'].apply(tokenize)\n","\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"fLhzvPVcGfDl"},"source":["**PROSES STOPWORD REMOVAL**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZl_jE8NGfDm"},"outputs":[],"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words = stopwords.words('indonesian')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob9T1S4jGfDm"},"outputs":[],"source":["def remove_stopwords(text):\n","    return [word for word in text if word not in stop_words]\n","\n","df['stopword removal'] = df['tokenize'].apply(lambda x: remove_stopwords(x))\n","\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"pKPcL4xGGfDm"},"source":["**PROSES STEAMING DATA**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTi7_vOCGfDm"},"outputs":[],"source":["!pip install Sastrawi\n","\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","from nltk.stem import PorterStemmer\n","from nltk.stem.snowball import SnowballStemmer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QOlG9hLGfDn"},"outputs":[],"source":["factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","def stem_text(text):\n","    return [stemmer.stem(word) for word in text]\n","\n","df['steming_data'] = df['stopword removal'].apply(lambda x: ' '.join(stem_text(x)))\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"eBXRbwGbGfDn"},"source":["**PROSES HAPUS DATA BERNILAI KOSONG (NAN)**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1c3Eo4KWGfDn"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoEUCvbpGfDn"},"outputs":[],"source":["data = df.dropna()\n","data.info()"]},{"cell_type":"markdown","metadata":{"id":"7mKVZG1aGfDo"},"source":["**WORDCLOUD SETELAH PREPROCESSING**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XU0W_OdsGfDo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import matplotlib.pyplot as plt\n","\n","# Menggabungkan teks dari kolom 'steming_data'\n","text = ' '.join(data['steming_data'].astype(str).tolist())\n","\n","stopwords = set(STOPWORDS)\n","stopwords.update([\n","    'abang', 'adik', 'adek', 'allah', 'amin', 'awas', 'banget', 'bawa', 'berat', 'biar', 'boys', 'cuman', 'cuy',\n","    'dan', 'dek', 'di', 'dik', 'download', 'duluan', 'efek', 'erick', 'full', 'ganti', 'gue', 'guardiola', 'hilang',\n","    'hoki', 'ini', 'jalan', 'jajar', 'kah', 'kali', 'kalah', 'kayak', 'ke', 'ku', 'lagu', 'lagi', 'latih', 'lawan',\n","    'lu', 'lupa', 'mafia', 'malaysia', 'nih', 'nya', 'pas', 'pecat', 'sih', 'sia', 'suka', 'susul', 'takut', 'tau',\n","    'tidur', 'tinggal', 'tohir', 'tolong', 'towel', 'tengah', 'tunggu', 'untung', 'vietnam', 'wasit', 'ya', 'yaman',\n","    'yang', 'yg', 'yuk'\n","])\n","\n","wc = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=500, width=800, height=400)\n","\n","wc.generate(text)\n","\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wc, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SP0Xb-lpGfDo"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","from wordcloud import STOPWORDS\n","\n","text = \" \".join(data[\"steming_data\"])\n","\n","stopwords = set(STOPWORDS)\n","stopwords.update([\n","    'abang', 'adik', 'adek', 'allah', 'amin', 'awas', 'banget', 'bawa', 'berat', 'biar', 'boys', 'cuman', 'cuy',\n","    'dan', 'dek', 'di', 'dik', 'download', 'duluan', 'efek', 'erick', 'full', 'ganti', 'gue', 'guardiola', 'hilang',\n","    'hoki', 'ini', 'jalan', 'jajar', 'kah', 'kali', 'kalah', 'kayak', 'ke', 'ku', 'lagu', 'lagi', 'latih', 'lawan',\n","    'lu', 'lupa', 'mafia', 'malaysia', 'nih', 'nya', 'pas', 'pecat', 'sih', 'sia', 'suka', 'susul', 'takut', 'tau',\n","    'tidur', 'tinggal', 'tohir', 'tolong', 'towel', 'tengah', 'tunggu', 'untung', 'vietnam', 'wasit', 'ya', 'yaman',\n","    'yang', 'yg', 'yuk'\n","])\n","\n","tokens = [word for word in text.split() if word not in stopwords]\n","word_counts = Counter(tokens)\n","\n","top_words = word_counts.most_common(10)\n","word, count = zip(*top_words)\n","# Menggunakan palet warna lebih soft\n","colors = plt.cm.Pastel1(range(len(word)))\n","\n","# Membuat plot\n","plt.figure(figsize=(12, 5))\n","bars = plt.bar(word, count, color=colors)\n","plt.xlabel(\"Kata-Kata Sering Muncul\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Jumlah Kata\", fontsize=12, fontweight='bold')\n","plt.title(\"Frekuensi Kata\", fontsize=18, fontweight='bold')\n","plt.xticks(rotation=45)\n","\n","# Menambahkan angka rata tengah di atas setiap bar\n","for bar, num in zip(bars, count):\n","    plt.text(bar.get_x() + bar.get_width() / 1.6 - 0.1, num + 1, str(num), fontsize=12, color='black', ha='center')\n","\n","# Menampilkan plot\n","plt.show()\n"]},{"cell_type":"code","source":["data.to_csv('hasil_preprocessing.csv',encoding='utf8', index=False)"],"metadata":{"id":"dGYc_UTycBTf"},"execution_count":null,"outputs":[]}]}